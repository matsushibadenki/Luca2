# **💻 MetaIntelligence CLIコマンドガイド (v1.0)**

MetaIntelligenceは、その高度なAIシステムと対話するための強力なコマンドラインインターフェース（CLI）を提供します。このガイドでは、利用可能なコマンド、オプション、そして実践的な使用例について詳しく説明します。

## **🚀 基本的なコマンド構造**

CLIの主要なエントリーポイントは fetch\_llm\_v2.py です。

python fetch\_llm\_v2.py \<provider\> "\<prompt\>" \[options\]

* **\<provider\>**: 使用するLLMプロバイダー (openai, claude, gemini, ollama, llamacpp, huggingface)  
* **"\<prompt\>"**: AIに解決してほしい質問やタスク

## **✨ 主要コマンドと推論モード**

### **1\. solve コマンド (問題解決)**

これがAIとの対話を行うための基本コマンドです。--modeオプションで推論戦略を指定します。

| モード | 対象の複雑性 | 主な利点 | 用途 |
| :---- | :---- | :---- | :---- |
| adaptive | 自動検出 | 動的な戦略最適化 | **推奨されるデフォルトモード**、複雑性が不明な質問 |
| efficient | 低 | 過剰思考の防止 | 簡単な質問、迅速な応答が必要なタスク |
| balanced | 中 | 最適な推論品質 | 標準的な分析、説明、要約 |
| decomposed | 高 | 思考の崩壊防止＆速度向上 | 複雑な問題解決、システム設計 |
| parallel | 全て | 複数戦略からの最良解選択 | 最高品質が求められる重要タスク |
| quantum\_inspired | 全て | 全体論的・統合的な洞察 | ブレインストーミング、哲学的問い、戦略立案 |
| speculative\_thought | 全て | 探索的なアイデア生成 | 初期段階のアイデア出し、多様な視点の獲得 |
| self\_discover | 未知 | 自律的な戦略構築 | 新規性の高い問題、明確な解決策がないタスク |
| edge | 低 | 軽量・高速 | リソースが限られたデバイス、簡単な確認 |
| paper\_optimized | 全て | 研究成果の完全統合 | 最高品質が求められるタスク、ベンチマーク |

### **2\. 使用例**

#### **適応モード（推奨）**

AIが問題の複雑性を自動で判断し、最適な戦略を選択します。

python fetch\_llm\_v2.py ollama "自己認識とは何か、簡潔に教えてください。" \--mode adaptive

#### **高複雑性モード**

複雑な問題を複数のサブ問題に分解して、高品質な回答を生成します。

python fetch\_llm\_v2.py openai "持続可能な都市交通システムの包括的な設計案を、技術、経済、社会、環境要因を考慮してタイムライン付きで作成してください。" \--mode decomposed

#### **自己発見モード**

AI自身が問題解決のための思考ステップを動的に組み立てます。

python fetch\_llm\_v2.py claude "AIにとって『創造性』とは何かを定義し、それを獲得するための方法論を提案してください。" \--mode self\_discover

## **📚 RAG (検索拡張生成) オプション**

外部の知識ソースを参照して、回答の精度と鮮度を高めます。

* \--rag: RAG機能を有効にします。  
* \--knowledge-base \<path\>: ローカルのファイル（PDF, TXTなど）を知識ソースとして指定します。  
* \--wikipedia: Wikipediaを知識ソースとして使用します。

### **使用例**

\# Wikipediaの情報を参照して回答  
python fetch\_llm\_v2.py openai "ジェイムズ・ウェッブ宇宙望遠鏡の最新の発見について教えて。" \--mode balanced \--wikipedia

\# ローカルのPDFレポートを要約  
\# 'my\_report.pdf' がカレントディレクトリに存在すると仮定  
python fetch\_llm\_v2.py claude "このレポートから、再生可能エネルギー導入の課題を要約してください。" \--mode balanced \--rag \--knowledge-base my\_report.pdf

## **🛠️ システム管理と診断コマンド**

システムのセットアップ状態を確認し、トラブルシューティングに役立てます。

* \--list-providers: 利用可能なプロバイダーの一覧を表示します。  
* \--system-status: V2モードを含む、システムの全体的な状態を表示します。  
* \--health-check: 指定したプロバイダーの接続状態やAPIキーの有効性を確認します。  
* \--troubleshooting: よくある問題とその解決策を表示します。

### **使用例**

\# 利用可能なプロバイダーを確認  
python fetch\_llm\_v2.py \--list-providers

\# Ollamaサーバーの接続状態とロード済みモデルを確認  
python fetch\_llm\_v2.py ollama \--health-check

## **⚙️ 一般的なオプション**

これらのオプションは、主要なコマンドと組み合わせて使用できます。

* \--model \<name\>: プロバイダーのデフォルトモデルを上書きします。  
* \-f, \--file \<path\>: プロンプトをコマンドラインからではなく、指定したファイルから読み込みます。  
* \--system-prompt "\<text\>": AIに特定の役割や指示を与えるシステムプロンプトを設定します。  
* \--temperature \<0.0-1.0\>: 回答の創造性・ランダム性を調整します。  
* \--max-tokens \<number\>: 回答の最大長（トークン数）を指定します。  
* \--json: 出力をJSON形式にします。思考プロセスなどの詳細なメタデータが含まれます。

### **使用例**

\# 特定のモデルと役割を指定して実行  
python fetch\_llm\_v2.py openai "光合成のプロセスを説明して。" \\  
  \--model gpt-4o-mini \\  
  \--system-prompt "あなたは植物学者で、好奇心旺盛な子供に説明しています。"

\# JSON形式で詳細な思考プロセスを確認  
python fetch\_llm\_v2.py ollama "AI倫理の主要な課題を3つ挙げてください。" \--mode balanced \--json

## **🔧 V2専用オプション**

V2の高度な機能を細かく制御するためのオプションです。

* \--force-v2: V2拡張モードでない場合でも、V2の処理パイプラインを強制的に使用します。  
* \--no-fallback: V2モードでの処理が失敗した際に、標準プロバイダーへのフォールバックを無効にします。  
* \--no-real-time-adjustment: adaptiveモードにおける、実行中の動的な複雑性再評価を無効にします。