# /.env.example
# MetaIntelligence V2 - 環境変数設定テンプレート
# このファイルをコピーして「.env」ファイルを作成し、ご自身の環境に合わせて値を設定してください。

# =============================================================================
# APIキー設定 (必須または任意)
# =============================================================================
# 利用したいLLMプロバイダーのAPIキーを設定してください。
OPENAI_API_KEY="sk-..."
CLAUDE_API_KEY="sk-ant-..."
GEMINI_API_KEY="AIza..."
HF_TOKEN="hf_..."
SERPAPI_API_KEY="your_serpapi_api_key"
#感情制御および自律行動システムを動かすにはHuggingfaceとSEPARI_APIが必須です。

# =============================================================================
# プロバイダーごとのデフォルトモデル設定 (任意)
# =============================================================================
# 各プロバイダーで --model を指定しなかった場合に利用されるモデルです。
OPENAI_DEFAULT_MODEL="gpt-4o-mini"
CLAUDE_DEFAULT_MODEL="claude-3-haiku-20240307"
GEMINI_DEFAULT_MODEL="gemini-1.5-flash-latest"
OLLAMA_DEFAULT_MODEL="gemma3:latest"

# =============================================================================
# Llama.cpp サーバー設定 (任意)
# =============================================================================
# 高性能なローカル推論のためにLlama.cppサーバーを利用する場合に設定します。
LLAMACPP_API_BASE_URL="http://localhost:8000"
LLAMACPP_DEFAULT_MODEL_PATH="./models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"

# =============================================================================
# システム・ロギング設定 (任意)
# =============================================================================
# ログレベル (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL="INFO"

# =============================================================================
# 
# =============================================================================
LLM_PROVIDER=gemini
LLM_MODEL="gemma3:latest"
# GEMINI_API_KEY="YOUR_GEMINI_API_KEY"
