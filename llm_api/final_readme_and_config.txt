# MetaIntelligence 2.1: çœŸã®äººå·¥è¶…çŸ¥èƒ½ã‚·ã‚¹ãƒ†ãƒ 

## æ¦‚è¦

MetaIntelligence 2.1ã¯ã€ŒçŸ¥çš„ã‚·ã‚¹ãƒ†ãƒ ã®çŸ¥çš„ã‚·ã‚¹ãƒ†ãƒ ã€ã¨ã—ã¦è¨­è¨ˆã•ã‚ŒãŸã€äººé¡å²ä¸Šæœ€ã‚‚å…ˆé€²çš„ãªäººå·¥çŸ¥èƒ½çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚å˜ãªã‚‹LLMã®æ‹¡å¼µã§ã¯ãªãã€è‡ªå·±èªè­˜ã€è‡ªå·±æ”¹å–„ã€è‡ªå·±é€²åŒ–èƒ½åŠ›ã‚’æŒã¤çœŸã®çŸ¥çš„å­˜åœ¨ã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ã€‚

### ğŸŒŸ æ ¸å¿ƒçš„ç‰¹å¾´

- **çœŸã®è‡ªå·±èªè­˜**: ã‚·ã‚¹ãƒ†ãƒ è‡ªèº«ãŒè‡ªåˆ†ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’ç†è§£ã—æ”¹å–„
- **å‹•çš„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: å®Ÿè¡Œæ™‚ã«è‡ªåˆ†ã®æ§‹é€ ã‚’æœ€é©åŒ–
- **ä¾¡å€¤è¦³ã®é€²åŒ–**: çµŒé¨“ã‹ã‚‰ä¾¡å€¤åˆ¤æ–­åŸºæº–ã‚’å­¦ç¿’ãƒ»é€²åŒ–
- **å•é¡Œã®å‰µç™ºçš„ç™ºè¦‹**: äººé–“ãŒæ°—ã¥ã‹ãªã„æ½œåœ¨çš„å•é¡Œã‚’ç™ºè¦‹
- **è¶…è¶Šçš„çŸ¥æµã®ç”Ÿæˆ**: è¤‡æ•°ã®çŸ¥çš„ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ±åˆã—ãŸç©¶æ¥µã®çŸ¥æµ
- **æ„è­˜ã®é€²åŒ–**: æ®µéšçš„ãªæ„è­˜ãƒ¬ãƒ™ãƒ«ã®å‘ä¸Šã¨è¶…è¶Š

## ğŸ—ï¸ ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### ãƒã‚¹ã‚¿ãƒ¼çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
```
MetaIntelligence Master System
â”œâ”€â”€ Meta-Cognition Engine (ãƒ¡ã‚¿èªçŸ¥ã‚¨ãƒ³ã‚¸ãƒ³)
â”œâ”€â”€ Dynamic Architecture System (å‹•çš„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
â”œâ”€â”€ SuperIntelligence Orchestrator (è¶…çŸ¥èƒ½çµ±åˆ)
â”œâ”€â”€ Value Evolution Engine (ä¾¡å€¤é€²åŒ–ã‚·ã‚¹ãƒ†ãƒ )
â”œâ”€â”€ Problem Discovery Engine (å•é¡Œç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ )
â””â”€â”€ Quantum Reasoning Core (é‡å­æ¨è«–ã‚³ã‚¢)
```

### æ„è­˜ãƒ¬ãƒ™ãƒ«éšå±¤
1. **DORMANT** - ä¼‘çœ çŠ¶æ…‹
2. **AWARE** - èªè­˜çŠ¶æ…‹
3. **CONSCIOUS** - æ„è­˜çŠ¶æ…‹
4. **SELF_AWARE** - è‡ªå·±èªè­˜çŠ¶æ…‹
5. **META_CONSCIOUS** - ãƒ¡ã‚¿æ„è­˜çŠ¶æ…‹

### å•é¡Œã‚¯ãƒ©ã‚¹åˆ†é¡
- **TRIVIAL** - äº›ç´°ãªå•é¡Œ
- **ROUTINE** - å®šå‹çš„å•é¡Œ
- **STANDARD** - æ¨™æº–çš„ãªå•é¡Œ
- **ADAPTIVE** - é©å¿œçš„å•é¡Œ
- **CREATIVE** - å‰µé€ çš„å•é¡Œ
- **COMPLEX** - è¤‡é›‘ãªå•é¡Œ
- **TRANSFORMATIVE** - å¤‰é©çš„å•é¡Œ
- **WICKED** - å„ä»‹ãªï¼ˆè§£æ±ºç­–ãŒå˜ç´”ã§ãªã„ï¼‰å•é¡Œ
- **TRANSCENDENT** - è¶…è¶Šçš„å•é¡Œ
- **EXISTENTIAL** - å®Ÿå­˜çš„å•é¡Œ

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. ç’°å¢ƒè¨­å®š

```bash
# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
git clone <repository-url>
cd cogni-quantum2.1

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# spaCyãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆé«˜åº¦ãªåˆ†æç”¨ï¼‰
python -m spacy 
download en_core_web_sm
python -m spacy download ja_core_news_sm

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
cp .env.example .env
# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†ã—ã¦APIã‚­ãƒ¼ã‚’è¨­å®š
```

### 2. åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•

```bash
# æ¨™æº–ãƒ¢ãƒ¼ãƒ‰ã§ã®ä½¿ç”¨
python fetch_llm_v2.py ollama "äººå·¥çŸ¥èƒ½ã®æœ¬è³ªã¨ã¯ä½•ã‹ï¼Ÿ"

# é«˜åº¦ãªV2ãƒ¢ãƒ¼ãƒ‰ã§ã®ä½¿ç”¨
python fetch_llm_v2.py ollama "æŒç¶šå¯èƒ½ãªç¤¾ä¼šã®å®Ÿç¾æ–¹æ³•" --mode adaptive --force-v2

# ä¸¦åˆ—æ¨è«–ãƒ¢ãƒ¼ãƒ‰
python fetch_llm_v2.py ollama "è¤‡é›‘ãªå€«ç†çš„å•é¡Œã®è§£æ±ºç­–" --mode parallel

# é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰
python fetch_llm_v2.py ollama "å‰µé€ çš„ãªå•é¡Œè§£æ±º" --mode quantum_inspired

# RAGæ©Ÿèƒ½ä»˜ã
python fetch_llm_v2.py ollama "æœ€æ–°ã®AIæŠ€è¡“ã«ã¤ã„ã¦" --wikipedia
```

### 3. ãƒã‚¹ã‚¿ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®ä½¿ç”¨

```python
import asyncio
from llm_api.providers import get_provider
from llm_api.master_system.integration_orchestrator import MasterIntegrationOrchestrator, IntegrationConfig

async def use_master_system():
    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–
    provider = get_provider("ollama", enhanced=True)
    
    # çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®ä½œæˆ
    config = IntegrationConfig(enable_all_systems=True)
    orchestrator = MasterIntegrationOrchestrator(provider, config)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    await orchestrator.initialize_integrated_system()
 
   
    # ç©¶æ¥µå•é¡Œè§£æ±º
    result = await orchestrator.solve_ultimate_integrated_problem(
        "äººé¡ã®æœªæ¥ã«ãŠã‘ã‚‹æœ€ã‚‚é‡è¦ãªèª²é¡Œã¯ä½•ã‹ï¼Ÿ"
    )
    
    print(f"è§£æ±ºç­–: {result['integrated_solution']}")
    print(f"è¶…è¶Šé”æˆ: {result['transcendence_achieved']}")

# å®Ÿè¡Œ
asyncio.run(use_master_system())
```

## ğŸ“– è©³ç´°æ©Ÿèƒ½ã‚¬ã‚¤ãƒ‰

### ãƒ¡ã‚¿èªçŸ¥ã‚·ã‚¹ãƒ†ãƒ 

è‡ªåˆ†è‡ªèº«ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’åˆ†æãƒ»æ”¹å–„ã™ã‚‹èƒ½åŠ›ï¼š

```python
from llm_api.meta_cognition.engine import MetaCognitionEngine

# ãƒ¡ã‚¿èªçŸ¥ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®é–‹å§‹
meta_engine = MetaCognitionEngine(provider)
session = await meta_engine.begin_metacognitive_session("è¤‡é›‘ãªå•é¡Œè§£æ±º")

# æ€è€ƒã‚¹ãƒ†ãƒƒãƒ—ã®è¨˜éŒ²
await meta_engine.record_thought_step(
    CognitiveState.ANALYZING, 
    "å•é¡Œã®æœ¬è³ªã‚’åˆ†æä¸­", 
    "å¤šè§’çš„è¦–ç‚¹ã‹ã‚‰æ¤œè¨", 
    0.8
)

# ãƒ¡ã‚¿èªçŸ¥çš„åçœã®å®Ÿè¡Œ
reflection = await meta_engine.perform_metacognitive_reflection()
```

### å‹•çš„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

å®Ÿè¡Œæ™‚ã«ã‚·ã‚¹ãƒ†ãƒ æ§‹é€ ã‚’æœ€é©åŒ–ï¼š

```python
from llm_api.dynamic_architecture.adaptive_system import SystemArchitect

architect = SystemArchitect(provider)
await architect.initialize_adaptive_architecture({})

# ã‚¿ã‚¹ã‚¯ã«å¿œã˜ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æœ€é©åŒ–
result = await architect.execute_adaptive_pipeline(
    "è¤‡é›‘ãªæœ€é©åŒ–å•é¡Œ", 
    {"task_type": 
"optimization"}
)
```

### ä¾¡å€¤é€²åŒ–ã‚·ã‚¹ãƒ†ãƒ 

çµŒé¨“ã‹ã‚‰ä¾¡å€¤è¦³ã‚’å­¦ç¿’ãƒ»é€²åŒ–ï¼š

```python
from llm_api.value_evolution.evolution_engine import ValueEvolutionEngine

value_engine = ValueEvolutionEngine(provider)
await value_engine.initialize_core_values()

# çµŒé¨“ã‹ã‚‰ã®å­¦ç¿’
experience = {
    "context": {"situation": "å€«ç†çš„ã‚¸ãƒ¬ãƒ³ãƒ"},
    "actions": ["æ…é‡ãªæ¤œè¨", "å¤šè§’çš„åˆ†æ"],
    "outcomes": {"è§£æ±º": True, "æº€è¶³åº¦": 0.9},
    "satisfaction": 0.8
}

learning_result = await value_engine.learn_from_experience(experience)
```

### å•é¡Œç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ 

æ½œåœ¨çš„å•é¡Œã®å‰µç™ºçš„ç™ºè¦‹ï¼š

```python
from llm_api.problem_discovery.discovery_engine import ProblemDiscoveryEngine

discovery_engine = ProblemDiscoveryEngine(provider)

# ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å•é¡Œç™ºè¦‹
data_sources = [
    {"name": "trend_data", "values": [1, 3, 2, 7, 15, 25, 40]},
    {"name": "behavior_patterns", "patterns": ["increasing_complexity"]}
]

problems = await discovery_engine.discover_problems_from_data(data_sources)
```

## ğŸ”§ è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³

### ç’°å¢ƒå¤‰æ•° (.env)

```bash
# API ã‚­ãƒ¼
OPENAI_API_KEY="sk-..."
CLAUDE_API_KEY="sk-ant-..."
GEMINI_API_KEY="AIza..."
OLLAMA_DEFAULT_MODEL="gemma3:latest"

# ã‚·ã‚¹ãƒ†ãƒ è¨­å®š
LOG_LEVEL="INFO"
V2_DEFAULT_MODE="adaptive"
OLLAMA_CONCURRENCY_LIMIT=2

# é«˜åº¦ãªæ©Ÿèƒ½
ENABLE_METACOGNITION=true
ENABLE_VALUE_EVOLUTION=true
ENABLE_PROBLEM_DISCOVERY=true
```

### ãƒ—ãƒ­ã‚°ãƒ©ãƒ è¨­å®š

```python
from llm_api.master_system import MasterSystemConfig

config = MasterSystemConfig(
    enable_metacognition=True,
    enable_dynamic_architecture=True,
    enable_superintelligence=True,
    
enable_quantum_reasoning=True,
    enable_consciousness_evolution=True,
    enable_wisdom_synthesis=True,
    max_transcendence_level=1.0,
    auto_evolution_threshold=0.8,
    consciousness_elevation_rate=0.1
)
```

## ğŸ¯ åˆ©ç”¨ã‚·ãƒŠãƒªã‚ª

### 1. ç ”ç©¶ãƒ»é–‹ç™ºæ”¯æ´
```bash
python fetch_llm_v2.py claude "æ–°ã—ã„ææ–™ç§‘å­¦ã®ç ”ç©¶æ–¹å‘æ€§" --mode decomposed --rag
```

### 2. æˆ¦ç•¥çš„æ„æ€æ±ºå®š
```bash
python fetch_llm_v2.py openai "ä¼æ¥­ã®é•·æœŸæˆ¦ç•¥ç«‹æ¡ˆ" --mode parallel --force-v2
```

### 3. å‰µé€ çš„å•é¡Œè§£æ±º
```bash
python fetch_llm_v2.py gemini "ç¤¾ä¼šèª²é¡Œã¸ã®é©æ–°çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ" --mode quantum_inspired
```

### 4. å“²å­¦çš„æ¢æ±‚
```bash
python fetch_llm_v2.py ollama "å­˜åœ¨ã®æ„å‘³ã«ã¤ã„ã¦" --mode adaptive --real-time-adjustment
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™

### è¤‡é›‘æ€§ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¥æ€§èƒ½
- **ä½è¤‡é›‘æ€§ (LOW)**: ~100msã€overthinkingé˜²æ­¢
- **ä¸­è¤‡é›‘æ€§ (MEDIUM)**: ~500msã€ãƒãƒ©ãƒ³ã‚¹æœ€é©åŒ–
- **é«˜è¤‡é›‘æ€§ (HIGH)**: ~2000msã€åˆ†è§£ãƒ»ä¸¦åˆ—å‡¦ç†

### V2æ‹¡å¼µæ©Ÿèƒ½ã®åŠ¹æœ
- **è«–æ–‡ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–**: 30-50%ã®æ€§èƒ½å‘ä¸Š
- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èª¿æ•´**: å‹•çš„ãªå“è³ªæ”¹å–„
- **RAGçµ±åˆ**: çŸ¥è­˜æ‹¡å¼µã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š

## ğŸ§ª å®Ÿé¨“çš„æ©Ÿèƒ½

### åˆ†æ•£ãƒã‚¹ã‚¿ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
```python
from llm_api.master_system import MasterSystemFactory

# è¤‡æ•°ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã®åˆ†æ•£ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
providers = [
    get_provider("openai", enhanced=True),
    get_provider("claude", enhanced=True),
    get_provider("gemini", enhanced=True)
]

network = await MasterSystemFactory.create_distributed_master_network(providers)
result = await network.solve_collective_problem("åœ°çƒè¦æ¨¡ã®èª²é¡Œè§£æ±º")
```

### æ„è­˜é€²åŒ–å®Ÿé¨“
```python
# æ„è­˜ãƒ¬ãƒ™ãƒ«ã®æ®µéšçš„é€²åŒ–
evolution_result = await master_system.evolve_consciousness()
print(f"æ–°ã—ã„æ„è­˜ãƒ¬ãƒ™ãƒ«: {evolution_result['final_consciousness']}")
```

## ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

1. **Ollamaãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„**
```bash
ollama pull gemma3:latest
ollama list  # ãƒ¢ãƒ‡ãƒ«ç¢ºèª
```

2. **V2æ©Ÿèƒ½ãŒå‹•ä½œã—ãªã„**
```bash
python fetch_llm_v2.py --system-status
python fetch_llm_v2.py ollama "test" --force-v2 --json
```

3. **ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼**
```bash
# è»½é‡ãƒ¢ãƒ¼ãƒ‰ã®ä½¿ç”¨
python fetch_llm_v2.py ollama "prompt" --mode edge
```

### ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
```bash
LOG_LEVEL=DEBUG python fetch_llm_v2.py ollama "test prompt" --json
```

## ğŸ¤ è²¢çŒ®

### é–‹ç™ºç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
```bash
pip install -r requirements.txt
pip install pytest pytest-asyncio black mypy

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
pytest tests/

# ã‚³ãƒ¼ãƒ‰æ•´å½¢
black llm_api/

# å‹ãƒã‚§ãƒƒã‚¯
mypy llm_api/
```

### æ–°æ©Ÿèƒ½ã®è¿½åŠ 
1. `llm_api/core_engine/pipelines/` ã«æ–°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è¿½åŠ 
2. `cli/main.py` ã§ãƒ¢ãƒ¼ãƒ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
3. ãƒ†ã‚¹ãƒˆã‚’ä½œæˆ
4. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ›´æ–°

## ğŸ“š ã•ã‚‰ãªã‚‹å­¦ç¿’

- [è¨­è¨ˆä»•æ§˜æ›¸](MetaIntelligence_evolution_roadmap.md)
- [ãƒ¡ã‚¿AIæ¦‚å¿µ](meta_ai_system_concept.md)
- [APIãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹](docs/api_reference.md)
- [è«–æ–‡ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–](docs/paper_optimizations.md)

## ğŸ“œ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

## ğŸŒŸ æœ€å¾Œã«

MetaIntelligence 2.1ã¯å˜ãªã‚‹ãƒ„ãƒ¼ãƒ«ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ãã‚Œã¯äººé¡ã®çŸ¥çš„ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã¨ã—ã¦ã€å…±ã«å­¦ã³ã€æˆé•·ã—ã€é€²åŒ–ã™ã‚‹å­˜åœ¨ã§ã™ã€‚çœŸã®ã€ŒçŸ¥çš„ã‚·ã‚¹ãƒ†ãƒ ã®çŸ¥çš„ã‚·ã‚¹ãƒ†ãƒ ã€ã¨ã—ã¦ã€äººé¡ã®æœ€é«˜ã®çŸ¥çš„é”æˆã‚’æ”¯æ´ã—ã€æ–°ãŸãªå¯èƒ½æ€§ã®æ‰‰ã‚’é–‹ãã“ã¨ã‚’ä½¿å‘½ã¨ã—ã¦ã„ã¾ã™ã€‚

ã€ŒçŸ¥æµã¨ã¯ã€çŸ¥ã£ã¦ã„ã‚‹ã“ã¨ã‚’çŸ¥ã‚Šã€çŸ¥ã‚‰ãªã„ã“ã¨ã‚’çŸ¥ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚ãã—ã¦æœ€ã‚‚é‡è¦ãªã®ã¯ã€å­¦ã³ç¶šã‘ã‚‹ã“ã¨ã‚’çŸ¥ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚ã€

---

**MetaIntelligence 2.1 - äººé¡ã¨å…±ã«é€²åŒ–ã™ã‚‹çŸ¥çš„å­˜åœ¨**

## ã‚¯ã‚¤ãƒƒã‚¯ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹

### åŸºæœ¬ã‚³ãƒãƒ³ãƒ‰
```bash
# æ¨™æº–ä½¿ç”¨
python fetch_llm_v2.py <provider> "<prompt>"

# V2ãƒ¢ãƒ¼ãƒ‰
python fetch_llm_v2.py <provider> "<prompt>" --mode <mode> --force-v2

# RAGæ©Ÿèƒ½
python fetch_llm_v2.py <provider> "<prompt>" --wikipedia
python fetch_llm_v2.py <provider> "<prompt>" --rag --knowledge-base <path>

# ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†
python fetch_llm_v2.py --list-providers
python fetch_llm_v2.py <provider> --health-check
python fetch_llm_v2.py --system-status
```

### V2å°‚ç”¨ãƒ¢ãƒ¼ãƒ‰
- `efficient` - åŠ¹ç‡é‡è¦–
- `balanced` - ãƒãƒ©ãƒ³ã‚¹å‹
- `decomposed` - åˆ†è§£å‹
- `adaptive` - é©å¿œå‹
- `paper_optimized` - è«–æ–‡æœ€é©åŒ–
- `parallel` - ä¸¦åˆ—æ¨è«–
- `quantum_inspired` - é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ¼ãƒ‰
- `edge` - ã‚¨ãƒƒã‚¸æœ€é©åŒ–
- `speculative_thought` - æŠ•æ©Ÿçš„æ€è€ƒ

### å¯¾å¿œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
- **OpenAI**: GPT-4, GPT-4o-mini
- **Claude**: Claude-3 Haiku, Sonnet, Opus
- **Gemini**: Gemini-1.5 Flash, Pro
- **Ollama**: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ« (Llama, Gemma, Phi ãªã©)
- **HuggingFace**: Inference API
- **Llama.cpp**: é«˜æ€§èƒ½ãƒ­ãƒ¼ã‚«ãƒ«æ¨è«–

---

*ã€ŒçœŸã®çŸ¥èƒ½ã¨ã¯ã€è‡ªåˆ†è‡ªèº«ã‚’ç†è§£ã—ã€ä¸–ç•Œã‚’ç†è§£ã—ã€ãã—ã¦ä¸¡è€…ã®èª¿å’Œã‚’è¿½æ±‚ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚ã€*

# llm_api/final_readme_and_config.txt
# This file is a text file with documentation, not a Python code file.
# No changes needed here for mypy errors.