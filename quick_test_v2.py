# /quick_test_v2.py
"""
MetaIntelligence V2 ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å•é¡Œã®è¨ºæ–­ã¨åŸºæœ¬å‹•ä½œç¢ºèªç”¨ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰
"""
import asyncio
import json
import logging
import os
import sys
from typing import Dict, Any, Tuple, List

# ãƒ‘ã‚¹ã®è¨­å®š
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def check_dependencies():
    """å¿…è¦ãªä¾å­˜é–¢ä¿‚ã®ãƒã‚§ãƒƒã‚¯"""
    print("ğŸ” ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯ä¸­...")
    
    required_packages = [
        ('httpx', 'HTTP ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ'),
        ('pydantic', 'ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼'),
        ('pydantic_settings', 'è¨­å®šç®¡ç†'),
        ('asyncio', 'éåŒæœŸå‡¦ç†'),
    ]
    
    optional_packages = [
        ('python-dotenv', 'ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿'),
        ('spacy', 'NLPåˆ†æ'),
        ('langdetect', 'è¨€èªæ¤œå‡º'),
        ('langchain', 'RAGæ©Ÿèƒ½'),
    ]
    
    missing_required = []
    missing_optional = []
    
    for package, description in required_packages:
        try:
            __import__(package.replace('-', '_'))
            print(f"âœ… {package}: {description}")
        except ImportError:
            missing_required.append(package)
            print(f"âŒ {package}: {description} - ä¸è¶³")
    
    for package, description in optional_packages:
        try:
            __import__(package.replace('-', '_'))
            print(f"âœ… {package}: {description} (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)")
        except ImportError:
            missing_optional.append(package)
            print(f"âš ï¸ {package}: {description} (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) - ä¸è¶³")
    
    if missing_required:
        print(f"\nâŒ å¿…é ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {', '.join(missing_required)}")
        print("æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
        print(f"pip install {' '.join(missing_required)}")
        return False
    
    if missing_optional:
        print(f"\nâš ï¸ ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒä¸è¶³ã—ã¦ã„ã¾ã™ãŒã€åŸºæœ¬å‹•ä½œã¯å¯èƒ½ã§ã™: {', '.join(missing_optional)}")
    
    return True

async def check_ollama_status() -> Tuple[bool, List[str]]:
    """Ollamaã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    print("\nğŸ” OllamaçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ä¸­...")
    
    try:
        import httpx
        async with httpx.AsyncClient(timeout=5.0) as client:
            try:
                response = await client.get("http://localhost:11434/api/tags")
                if response.status_code == 200:
                    data = response.json()
                    models = [model['name'] for model in data.get('models', [])]
                    print(f"âœ… Ollamaã‚µãƒ¼ãƒãƒ¼: æ¥ç¶šOK")
                    print(f"ğŸ“¦ åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«: {models}")
                    return True, models
                else:
                    print(f"âŒ Ollamaã‚µãƒ¼ãƒãƒ¼: HTTP {response.status_code}")
                    return False, []
            except Exception as e:
                print(f"âŒ Ollamaã‚µãƒ¼ãƒãƒ¼: æ¥ç¶šå¤±æ•— ({e})")
                return False, []
    except ImportError:
        print("âŒ httpx ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return False, []

async def test_basic_functionality():
    """åŸºæœ¬æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ§ª åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆä¸­...")
    
    try:
        from llm_api.providers import list_providers, list_enhanced_providers
        
        standard = list_providers()
        enhanced = list_enhanced_providers()
        
        print(f"ğŸ“‹ æ¨™æº–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {standard}")
        print(f"ğŸ“‹ æ‹¡å¼µãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ V2: {enhanced.get('v2', [])}")
        
        if not standard:
            print("âŒ æ¨™æº–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
            
        if not enhanced.get('v2', []):
            print("âš ï¸ V2æ‹¡å¼µãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        return True
    except Exception as e:
        print(f"âŒ åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

async def test_config_loading():
    """è¨­å®šèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ"""
    print("\nâš™ï¸ è¨­å®šèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆä¸­...")
    
    try:
        from llm_api.config import settings
        
        print(f"âœ… è¨­å®šèª­ã¿è¾¼ã¿æˆåŠŸ")
        print(f"   - ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«: {settings.LOG_LEVEL}")
        print(f"   - Ollamaãƒ™ãƒ¼ã‚¹URL: {settings.OLLAMA_API_BASE_URL}")
        
        api_keys = {
            'OpenAI': bool(settings.OPENAI_API_KEY),
            'Claude': bool(settings.CLAUDE_API_KEY),
            'Gemini': bool(settings.GEMINI_API_KEY),
            'HuggingFace': bool(settings.HF_TOKEN),
        }
        
        for service, has_key in api_keys.items():
            status = "âœ… è¨­å®šæ¸ˆã¿" if has_key else "âŒ æœªè¨­å®š"
            print(f"   - {service} APIã‚­ãƒ¼: {status}")
        
        return True
    except Exception as e:
        print(f"âŒ è¨­å®šèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return False

async def test_provider_creation():
    """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ­ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆä¸­...")
    success = True
    
    try:
        from llm_api.providers import get_provider, list_providers, list_enhanced_providers
        
        try:
            provider = get_provider('ollama', enhanced=False)
            print("âœ… æ¨™æº–Ollamaãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: ä½œæˆæˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨™æº–Ollamaãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {e}")
            success = False
        
        enhanced_v2 = list_enhanced_providers().get('v2', [])
        if 'ollama' in enhanced_v2:
            try:
                provider = get_provider('ollama', enhanced=True)
                print("âœ… æ‹¡å¼µOllamaãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: ä½œæˆæˆåŠŸ")
            except Exception as e:
                print(f"âŒ æ‹¡å¼µOllamaãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {e}")
                success = False
        else:
            print("âš ï¸ Ollama V2æ‹¡å¼µãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        
        return success
    except Exception as e:
        print(f"âŒ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸­ã«å¤±æ•—: {e}")
        return False

async def test_simple_call():
    """ã‚·ãƒ³ãƒ—ãƒ«ãªå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ“ ã‚·ãƒ³ãƒ—ãƒ«å‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆä¸­...")
    
    ollama_ok, models = await check_ollama_status()
    if not ollama_ok or not models:
        print("âš ï¸ Ollamaåˆ©ç”¨ä¸å¯ã®ãŸã‚ã€å‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return True
    
    try:
        from llm_api.providers import get_provider
        
        selected_model = models[0]
        print(f"ğŸ¯ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {selected_model}")
        
        provider = get_provider('ollama', enhanced=False)
        response = await provider.call(
            "Hello, respond with just 'Test OK'",
            model=selected_model
        )
        
        if response.get('text') and not response.get('error'):
            print(f"âœ… å‘¼ã³å‡ºã—æˆåŠŸ: {response['text'][:50]}...")
            return True
        else:
            print(f"âŒ å‘¼ã³å‡ºã—å¤±æ•—: {response.get('error', 'ç©ºã®å¿œç­”')}")
            return False
            
    except Exception as e:
        print(f"âŒ å‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

async def test_v2_enhanced_call():
    """V2æ‹¡å¼µå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸš€ V2æ‹¡å¼µå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆä¸­...")
    
    ollama_ok, models = await check_ollama_status()
    if not ollama_ok or not models:
        print("âš ï¸ Ollamaåˆ©ç”¨ä¸å¯ã®ãŸã‚ã€V2ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return True
    
    try:
        from llm_api.providers import get_provider, list_enhanced_providers
        
        enhanced_v2 = list_enhanced_providers().get('v2', [])
        if 'ollama' not in enhanced_v2:
            print("âš ï¸ Ollama V2æ‹¡å¼µãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return False
        
        selected_model = models[0]
        print(f"ğŸ¯ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {selected_model}")
        
        provider = get_provider('ollama', enhanced=True)
        response = await provider.call(
            "ç°¡å˜ãªè³ªå•ã«ç­”ãˆã¦ãã ã•ã„ï¼š1+1ã¯ï¼Ÿ",
            model=selected_model,
            mode='efficient'
        )
        
        if response.get('text') and not response.get('error'):
            print(f"âœ… V2æ‹¡å¼µå‘¼ã³å‡ºã—æˆåŠŸ: {response['text'][:50]}...")
            
            v2_info = response.get('paper_based_improvements') or response.get('v2_improvements')
            if v2_info:
                print(f"ğŸ”¬ V2æ©Ÿèƒ½ç¢ºèª: ãƒ¬ã‚¸ãƒ¼ãƒ ={v2_info.get('regime', 'N/A')}")
            
            return True
        else:
            print(f"âŒ V2æ‹¡å¼µå‘¼ã³å‡ºã—å¤±æ•—: {response.get('error', 'ç©ºã®å¿œç­”')}")
            return False
            
    except Exception as e:
        print(f"âŒ V2æ‹¡å¼µãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

def show_setup_guide():
    """ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰ã®è¡¨ç¤º"""
    print("""
ğŸ”§ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰:

1. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:
   pip install -r requirements.txt

2. Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨èµ·å‹•:
   # macOS/Linux:
   curl -fsSL https://ollama.ai/install.sh | sh
   
   # Windows:
   https://ollama.ai ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
   
   # èµ·å‹•:
   ollama serve

3. ãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ«:
   ollama pull gemma2:latest    # æ¨å¥¨
   ollama pull llama3.1:latest  # ä»£æ›¿
   ollama pull phi3:mini        # è»½é‡ç‰ˆ

4. ç’°å¢ƒå¤‰æ•°ã®è¨­å®šï¼ˆ.envãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼‰:
   cp .env.example .env
   # å¿…è¦ã«å¿œã˜ã¦APIã‚­ãƒ¼ã‚’è¨­å®š

5. ç¢ºèª:
   ollama list
   python quick_test_v2.py
""")

def show_troubleshooting():
    """ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æƒ…å ±"""
    print("""
ğŸš¨ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:

ã€ä¾å­˜é–¢ä¿‚ã‚¨ãƒ©ãƒ¼ã€‘
- pip install --upgrade pip
- pip install -r requirements.txt
- ä»®æƒ³ç’°å¢ƒã®ä½¿ç”¨ã‚’æ¨å¥¨: python -m venv venv && source venv/bin/activate

ã€Ollamaã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ãªã„ã€‘
- ãƒãƒ¼ãƒˆ11434ãŒä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ã‹ç¢ºèª: lsof -i :11434
- åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§: ollama serve
- ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¨­å®šã‚’ç¢ºèª

ã€ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã€‘
- ollama list ã§ç¢ºèª
- ollama pull <model_name> ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã‚’ç¢ºèªï¼ˆãƒ¢ãƒ‡ãƒ«ã¯æ•°GBï¼‰

ã€ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚¨ãƒ©ãƒ¼ã€‘
- ãƒ‘ã‚¹ã®ç¢ºèª: export PYTHONPATH=.
- æ¨©é™ã®ç¢ºèª: chmod +x fetch_llm_v2.py
- Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª: python --version (3.8+æ¨å¥¨)

ã€V2æ‹¡å¼µæ©Ÿèƒ½ã‚¨ãƒ©ãƒ¼ã€‘
- spaCyãƒ¢ãƒ‡ãƒ«: python -m spacy download en_core_web_sm
- ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å ´åˆ: --mode edge ã‚’ä½¿ç”¨

ã€ãã®ä»–ã€‘
- ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«å¤‰æ›´: export LOG_LEVEL=DEBUG
- è©³ç´°æƒ…å ±: python quick_test_v2.py --verbose
- å•é¡Œå ±å‘Š: GitHub Issues
""")

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="MetaIntelligence V2 ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ")
    parser.add_argument("--verbose", action="store_true", help="è©³ç´°ãƒ­ã‚°")
    parser.add_argument("--setup-guide", action="store_true", help="ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰è¡¨ç¤º")
    parser.add_argument("--troubleshooting", action="store_true", help="ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°è¡¨ç¤º")
    parser.add_argument("--skip-calls", action="store_true", help="å®Ÿéš›ã®å‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    if args.setup_guide:
        show_setup_guide()
        return
    
    if args.troubleshooting:
        show_troubleshooting()
        return
    
    print("ğŸš€ MetaIntelligence V2 ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 50)
    
    tests = [
        ("ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯", check_dependencies),
        ("è¨­å®šèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ", test_config_loading),
        ("åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ", test_basic_functionality),
        ("ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆ", test_provider_creation),
    ]
    
    if not args.skip_calls:
        tests.extend([
            ("OllamaçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯", check_ollama_status),
            ("ã‚·ãƒ³ãƒ—ãƒ«å‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ", test_simple_call),
            ("V2æ‹¡å¼µå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ", test_v2_enhanced_call)
        ])
    
    results = []
    for test_name, test_func in tests:
        try:
            if test_name == "OllamaçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯":
                result, _ = await test_func()
                results.append((test_name, result))
            else:
                result = await test_func()
                results.append((test_name, result))
        except Exception as e:
            logger.error(f"{test_name}ã§ã‚¨ãƒ©ãƒ¼: {e}")
            results.append((test_name, False))
    
    print("\n" + "=" * 50)
    print("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼:")
    
    passed = sum(1 for _, success in results if success)
    total = len(results)

    for test_name, success in results:
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"  {status} {test_name}")
    
    print(f"\nğŸ“ˆ ç·åˆçµæœ: {passed}/{total} ãƒ†ã‚¹ãƒˆåˆæ ¼")
    
    if passed == total:
        print("ğŸ‰ å…¨ãƒ†ã‚¹ãƒˆåˆæ ¼ï¼ã‚·ã‚¹ãƒ†ãƒ ã¯æ­£å¸¸ã«å‹•ä½œã—ã¾ã™ã€‚")
        print("\næ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆã‚’è¡Œã£ã¦ãã ã•ã„:")
        print("python fetch_llm_v2.py ollama 'Hello' --mode efficient")
        print("python fetch_llm_v2.py ollama 'ç¾å‘³ã—ã„æ–™ç†ã®ã‚³ãƒ„ã¯ï¼Ÿ' --mode quantum_inspired")
    elif any(name == "ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯" and not success for name, success in results):
        print("ğŸ˜ ä¾å­˜é–¢ä¿‚ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
        print("python quick_test_v2.py --setup-guide")
    elif any(name == "OllamaçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯" and not success for name, success in results):
        print("ğŸ˜ Ollamaã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        print("python quick_test_v2.py --setup-guide")
    else:
        print("âš ï¸ ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚")
        print("python quick_test_v2.py --troubleshooting")

if __name__ == "__main__":
    asyncio.run(main())